{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "4651ab91-b868-401f-8d29-91ac5efa060a"
    }
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbpresent": {
     "id": "d64c46d3-840a-49b3-a869-bc9868b4cdc6"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # 继承nn.Module\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__() # 等价于nn.Module.__init__(self)\n",
    "        self.w = nn.Parameter(t.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # x.@(self.w)\n",
    "        return x + self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "48fa3e9c-1da6-4739-b874-9108f871afea"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4755 -3.5846 -2.5601\n",
       " 0.7121 -2.5209 -3.7774\n",
       "[torch.FloatTensor of size 2x3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Linear(4,3)\n",
    "input = V(t.randn(2,4))\n",
    "output = layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "40a8150b-17ab-4aed-8e7c-c67c44f7af4d"
    }
   },
   "outputs": [],
   "source": [
    "for name,parameter in layer.named_parameters():\n",
    "    print(name,parameter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2bfb34ad-c0bc-4cfe-8624-5c68bc2b77c0"
    }
   },
   "outputs": [],
   "source": [
    "nn.Linear??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62c1ea01-814b-4b2b-aee1-2d37ebdcea6e"
    }
   },
   "source": [
    "# 指南：如何使用nn.Moudle搭建自己的神经网络模型\n",
    "\n",
    "- 首先，将tutorial中的中nn.Moudle的示例程序拷贝过来\n",
    "- 之后，先修改__init__()函数，具体需要修改的内容有如下几点：\n",
    "  1. 首先修改super函数，将第一个参数换成你设置的类名\n",
    "  2. 在__init__()函数中定义可学习的参数（就是可以通过反传更新的参数），将其封装成parameter可以采用nn.Parameter定义\n",
    "  3. PyTorch实现了神经网络中绝大多数的layer，这些layer都继承于nn.Module，封装了可学习参数parameter，并实现了forward函数。因此，大多数时候直接调用nn中的layer如nn.Linear构造神经网络层，如不需要自己用nn.Parameter定义参数.\n",
    "  4. 在直接调用nn中的layer构造神经网络层时，采用nn.Linear??命令查看这些函数的输入输出参数的意义，之后，根据自己神经网络的的构造确定输入输出参数。\n",
    "  5. 最后，根据自己定义的参数确定__init__()函数是否要接收除self以外的参数\n",
    "  6. 各种层的调用位置与名称：https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/\n",
    "- 之后，定义前向传播函数forward()，将不需要学习的参数都放在这里。注意__init__()函数可学习的的参数的层采用nn下的网络层class定义,forward()下的没有学习的的参数的层采用torch.nn.functional下的函数定义\n",
    "- 待续\n",
    "\n",
    "相关技巧汇总\n",
    "- 常用的层：卷积层（Conv）、池化层（Pool），逆卷积（TransposeConv），Linear：全连接层，BatchNorm：批规范化层，Dropout：dropout层\n",
    "- 可以采用ModuleList和Sequential采用类似keras的搭积木的方式搭网络，方法见下方示例\n",
    "- 网络的可学习参数通过`net.parameters()`返回，`net.named_parameters`可同时返回可学习的参数及名称。\n",
    "- PyTorch实现了常见的激活函数，其具体的接口信息可参见官方文档[^3]，这些激活函数可作为独立的layer使用。\n",
    "- 若反传的节点为张量而不是标量，需要设置grad_variables形状与节点维度一致，egy.backward(t.ones(y.size()))\n",
    "- 初始化见http://219.216.87.170:9999/notebooks/pytorch-book/chapter4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1nn/chapter4.ipynb 的4.4节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bfe50941-64b4-4139-8545-cef4774acbb9"
    }
   },
   "source": [
    "## tutorial中的中nn.Moudle的示例程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e1d99bb4-68af-423a-be3e-d9bb2bab915f"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e63a2e64-2c2b-452b-b68e-050322281b43"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module): # 继承nn.Module\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__() # 等价于nn.Module.__init__(self)\n",
    "        self.w = nn.Parameter(t.randn(in_features, out_features))\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w) # x.@(self.w)\n",
    "        return x + self.b.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "399c1770-d742-4ca0-993f-0dc132e4539c"
    }
   },
   "source": [
    "## Sequential的三种写法示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "346c2925-320d-438d-891f-9a7835c08552"
    }
   },
   "outputs": [],
   "source": [
    "# Sequential的三种写法\n",
    "net1 = nn.Sequential()\n",
    "net1.add_module('conv', nn.Conv2d(3, 3, 3))\n",
    "net1.add_module('batchnorm', nn.BatchNorm2d(3))\n",
    "net1.add_module('activation_layer', nn.ReLU())\n",
    "\n",
    "net2 = nn.Sequential(\n",
    "        nn.Conv2d(3, 3, 3),\n",
    "        nn.BatchNorm2d(3),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "from collections import OrderedDict\n",
    "net3= nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(3, 3, 3)),\n",
    "          ('bn1', nn.BatchNorm2d(3)),\n",
    "          ('relu1', nn.ReLU())\n",
    "        ]))\n",
    "print('net1:', net1)\n",
    "print('net2:', net2)\n",
    "print('net3:', net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0cca6442-6aa3-44d2-a48f-8ca0ffecb549"
    }
   },
   "source": [
    "## ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f761a7c8-c1d0-4770-a71b-72ba30421db4"
    }
   },
   "outputs": [],
   "source": [
    "modellist = nn.ModuleList([nn.Linear(3,4), nn.ReLU(), nn.Linear(4,2)])\n",
    "input = V(t.randn(1, 3))\n",
    "for model in modellist:\n",
    "    input = model(input)\n",
    "# 下面会报错,因为modellist没有实现forward方法\n",
    "# output = modelist(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e981b4d6-f86b-4bef-bf8d-b8b82cfd71f0"
    }
   },
   "source": [
    "# 如何写损失函数\n",
    "\n",
    "方法可见如下代码作为参考，\n",
    "\n",
    "在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可看作是一种特殊的layer，PyTorch也将这些损失函数实现为`nn.Module`的子类。然而在实际使用中通常将这些loss function专门提取出来，和主模型互相独立。详细的loss使用请参照文档[^5]，这里以分类中最常用的交叉熵损失CrossEntropyloss为例说明。\n",
    "[^5]: http://pytorch.org/docs/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = Variable(t.arange(0,10))  \n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3ea38b90-1704-4461-bc6a-fda912f1600d"
    }
   },
   "source": [
    "# pytorch优化器使用方法：\n",
    "- 首先，import torch.optim as optim\n",
    "- 之后，创建你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。\n",
    "  \n",
    "  eg:\n",
    "  optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "  optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
    "  各种优化器见;https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/\n",
    "  注意，可以采用以下例子给不同子网络设置不同的学习率\n",
    "- 之后，计算损失\n",
    "  output = net(intput)\n",
    "  criterion=nn.MSELoss()\n",
    "  loss=criterion(output,target)\n",
    "- 之后：在反传之前，将梯度清零：optim.zero_grad()。对于某个变量的梯度清零采用，variable.grad.data.zero_()\n",
    "- 之后，反向传播：loss.backward()   \n",
    "- 之后利用optimizer.step()更新参数\n",
    "\n",
    "对于如何调整学习率，主要有两种做法。一种是修改optimizer.param_groups中对应的学习率，另一种是更简单也是较为推荐的做法——新建优化器，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b272b9bc-2eef-460c-8151-0287cce2fd84"
    }
   },
   "outputs": [],
   "source": [
    "from torch import  optim\n",
    "# 调整学习率，新建一个optimizer\n",
    "old_lr = 0.1\n",
    "optimizer =optim.SGD([\n",
    "                {'params': net.features.parameters()},\n",
    "                {'params': net.classifier.parameters(), 'lr': old_lr*0.1}\n",
    "            ], lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "beca514d-3bf8-4a6a-aa2e-3ef51a47ffd1"
    }
   },
   "source": [
    "# 应该什么时候使用nn.Module，什么时候使用nn.functional呢？\n",
    "\n",
    "- 如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异\n",
    "- dropout操作也没有可学习操作，但建议还是使用`nn.Dropout`而不是`nn.functional.dropout`，因为dropout在训练和测试两个阶段的行为有所差别，使用`nn.Module`对象能够通过`model.eval`操作加以区分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45711565-51a9-46e2-984f-1ba9c34ef6af"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pool(F.relu(self.conv1(x)), 2)\n",
    "        x = F.pool(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9dc0f5d6-9a2b-441f-90a1-01ecb836a670"
    }
   },
   "source": [
    "# 初始化\n",
    "\n",
    "可以利用nn.init进行初始化。其中，nn.module模块大多采用较合理的初始化措施，不需要我们初始化，但使用nn.parameter时需要进行初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46062514-9f50-4435-894d-519e1b60642d"
    }
   },
   "source": [
    "# 训练与测试时代码需要修改的地方\n",
    "\n",
    "1. 对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试时不将其training值设为True，则可能会有很大影响，这在实际使用中要千万注意。虽然可通过直接设置`training`属性，来将子module设为train和eval模式，但这种方式较为繁琐，因如果一个模型具有多个dropout层，就需要为每个dropout层指定training属性。更为推荐的做法是调用`model.train()`函数，它会将当前module及其子module中的所有training属性都设为True，相应的，`model.eval()`函数会把training属性都设为False。\n",
    "2. 待续\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取中间变量的方法\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`register_forward_hook`与`register_backward_hook`，这两个函数的功能类似于variable函数的`register_hook`，可在module前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：`hook(module, input, output) -> None`，而反向传播则具有如下形式：`hook(module, grad_input, grad_output) -> Tensor or None`。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在forward函数中，但如果在forward函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。\n",
    "```python\n",
    "model = VGG()\n",
    "features = t.Tensor()\n",
    "def hook(module, input, output):\n",
    "    '''把这层的输出拷贝到features中'''\n",
    "    features.copy_(output.data)\n",
    "    \n",
    "handle = model.layer8.register_forward_hook(hook)\n",
    "_ = model(input)\n",
    "# 用完hook后删除\n",
    "handle.remove()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "t.save(net.state_dict(), 'net.pth')\n",
    "\n",
    "# 加载已保存的模型\n",
    "net2 = Net()\n",
    "net2.load_state_dict(t.load('net.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将Module放在GPU上运行也十分简单，只需两步：\n",
    "- model = model.cuda()：将模型的所有参数转存到GPU\n",
    "- input.cuda()：将输入数据也放置到GPU上\n",
    "\n",
    "至于如何在多个GPU上并行计算，PyTorch也提供了两个函数，可实现简单高效的并行GPU计算\n",
    "- nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)\n",
    "- class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "\n",
    "可见二者的参数十分相似，通过`device_ids`参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者则返回一个新的module，能够自动在多GPU上进行并行加速。\n",
    "\n",
    "```\n",
    "# method 1\n",
    "new_net = nn.DataParallel(net, device_ids=[0, 1])\n",
    "output = new_net(input)\n",
    "\n",
    "# method 2\n",
    "output = nn.parallel.data_parallel(new_net, input, device_ids=[0, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn和autograd的关系：\n",
    "\n",
    "nn.Module利用的也是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的Variable进行的各种操作，本质上都是用到了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别：\n",
    "- autograd.Function利用了Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播\n",
    "- nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播\n",
    "- nn.functional是一些autograd操作的集合，是经过封装的函数\n",
    "\n",
    "作为两大类扩充PyTorch接口的方法，我们在实际使用中应该如何选择呢？如果某一个操作，在autograd中尚未支持，那么只能实现Function接口对应的前向传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，正如第三章所实现的`Sigmoid`一样，比直接利用autograd低级别的操作要快。而如果只是想在深度学习中增加某一层，使用nn.Module进行封装则更为简单高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorchpy36]",
   "language": "python",
   "name": "conda-env-pytorchpy36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
