{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 构建 5x3 矩阵，输出维度，查看列的个数（两种写法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.Tensor(5,3)\n",
    "print(x)\n",
    "print(x.size())\n",
    "print(x.size(1))\n",
    "print(x.size()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((5,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用[0,1]均匀分布随机初始化二维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.size是tuple的子类，因此它支持tuple的所有操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法的三种写法，这里对张量赋值是拷贝不是引用，python对列表好像是引用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.add(y))\n",
    "print(torch.add(x,y))\n",
    "result=torch.Tensor(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add(）与add_()\n",
    "有区别一个是相加不改变加数的值，另一个则会改变\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.add_(y))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改tensor的size\n",
    "z=torch.randn(4,4)\n",
    "a=z.view(2,8)\n",
    "b=z.view(-1,8)\n",
    "print(a.size(),b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor可以转换为numpy的数组做处理再转换回tensor，尝试把tensor转换成numpy的array以及把numpy数组转换为tensor（其中转换前后的结果是共享内存的，一个变了另一个也要变）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.Tensor(5,3)\n",
    "b=a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       " 1  1\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.DoubleTensor of size 5x2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "c=np.ones((5,2))\n",
    "d=torch.from_numpy(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor可通过cuda转换为GPU的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    x=x.cuda()\n",
    "    y=y.cuda()\n",
    "    x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tensor新建一个变量，对变量各个元素求和，让求和结果对变量求导\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Variable(torch.ones(2,2),requires_grad=True)\n",
    "y=x+2\n",
    "z=3*(y**2)\n",
    "o=z.mean()\n",
    "o.grad_fn\n",
    "o.backward(retain_graph = True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于grad在反向传播的过程中是累加的（每次运行反向传播代码都会累加一次）\n",
    "所以，在进行反向传播之前要把梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.data.zero_()\n",
    "o.backward(retain_graph = True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variable与tensor的区别\n",
    "variable其中包含data，grad作为tensor以及函数grad_fn\n",
    "直接输出variable就是输出其data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继承可以把父类的所有功能都直接拿过来，这样就不必重零做起，子类只需要新增自己特有的方法，也可以把父类不适合的方法覆盖重写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "子类可以继承父类的的类间函数等功能，同时，也可以通过新增自己与父类同名的功能，从而在调用时覆盖掉父类的功能\n",
    "用isinstance()判断类型时子类既属于父类也属于其自身。所以在将子类作为变量传入调用父类类间函数的函数时会实现子类中同名的类间函数（假如有的话）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#步骤：把把网络中具有可学习参数的层放在构造函数`__init__`中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放。\n",
    "#但建议不放在其中，而在forward中使用`nn.functional`代替。\n",
    "#构造类Net,把可学习的参数的层放在init函数（构造函数）里定义，而把那些不具有可学习参数的层如RELU等放在类内的其他函数里\n",
    "#需要用到的函数： nn.Conv2d，nn.Linear，F.max_pool2d，F.relu\n",
    "# 卷积 -> 激活 -> 池化 ->全连接\n",
    "#各种层的调用位置与名称：https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "# 设置默认的level为DEBUG\n",
    "# 设置log的格式\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"[%(asctime)s] %(name)s:%(levelname)s: %(message)s\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,6,5)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.conv3=nn.Conv2d(16,120,5)\n",
    "        self.fc1=nn.Linear(120,84)\n",
    "        self.fc2=nn.Linear(84,10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        #当池化窗口是2*2等正方形的时候，我们可以在kernel_size处输入2取代输入（2，2）来简化                                       \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        \n",
    "        x=  F.relu(self.conv3(x))\n",
    "        x=  x.view(1,-1)\n",
    "        logging.debug(x.size())\n",
    "        #print(x.size())\n",
    "        x=  F.relu(self.fc1(x))\n",
    "        #print(x.size())\n",
    "        x=  self.fc2(x)\n",
    "        #print(x.size())\n",
    "        logging.debug(x.size())\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "conv1.weight : torch.Size([6, 1, 5, 5])\n",
      "conv1.bias : torch.Size([6])\n",
      "conv2.weight : torch.Size([16, 6, 5, 5])\n",
      "conv2.bias : torch.Size([16])\n",
      "conv3.weight : torch.Size([120, 16, 5, 5])\n",
      "conv3.bias : torch.Size([120])\n",
      "fc1.weight : torch.Size([84, 120])\n",
      "fc1.bias : torch.Size([84])\n",
      "fc2.weight : torch.Size([10, 84])\n",
      "fc2.bias : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for name,parameters in net.named_parameters():\n",
    "    print(name, ':',parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-27 10:44:25,751] root:DEBUG: torch.Size([1, 120])\n",
      "[2018-03-27 10:44:25,753] root:DEBUG: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward函数的输入和输出都是Variable，只有Variable才具有自动求导功能，而Tensor是没有的，所以在输入时，需把Tensor封装成Variable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-27 10:43:26,541] root:DEBUG: torch.Size([1, 120])\n",
      "[2018-03-27 10:43:26,543] root:DEBUG: torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "out.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出Net类中参数的个数，以及输出有哪些参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#输入网络，实现正向反向传播\n",
    "net.zero_grad() # 所有参数的梯度清零\n",
    "out.backward(Variable(torch.ones(1,10)),retain_graph=True) # 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算网络的MSE损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-27 10:43:50,851] root:DEBUG: torch.Size([1, 120])\n",
      "[2018-03-27 10:43:50,853] root:DEBUG: torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 28.6425\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(input)\n",
    "target=Variable(torch.arange(0,10))\n",
    "criterion=nn.MSELoss()#nn.MSELoss应该也是一个可以返回参数的类，和net一样，需要确定其实例才能开始运算\n",
    "#面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，\n",
    "#每个对象都拥有相同的方法，但各自的数据可能不同。\n",
    "loss=criterion(output,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() # 把net中所有可学习参数的梯度清零\n",
    "print('反向传播之前 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('反向传播之后 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器，pytorch优化器使用方法：\n",
    "首先，import torch,optim as optim\n",
    "之后，创建你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。\n",
    "eg:\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
    "各种优化器见;\n",
    "https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/\n",
    "之后：在训练过程中，将梯度清零\n",
    "optim.zero_grad()\n",
    "之后，计算损失\n",
    "output = net(intput)\n",
    "criterion=nn.MSELoss()\n",
    "loss=criterion(output,target)\n",
    "之后，反向传播\n",
    "loss.backward()   \n",
    "之后利用optimizer.step更新参数\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
